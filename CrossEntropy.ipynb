{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNu2NW555ltPbNys9v13C0I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reinanovazania/reinanovazania/blob/main/CrossEntropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V61mZppQ3lMt",
        "outputId": "112efcde-8b97-4d92-a21e-442d73dfb1d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 11)\n"
          ]
        }
      ],
      "source": [
        "#reina novazania\n",
        "#crossEntropy\n",
        "import numpy as np\n",
        "# StringIO behaves like a file object\n",
        "from io import StringIO  \n",
        "\n",
        "import matplotlib.pyplot as plt # matplotlib is for plotting\n",
        "%matplotlib inline\n",
        "\n",
        "# import autograd functionality\n",
        "import autograd.numpy as np\n",
        "from autograd import value_and_grad \n",
        "from autograd import hessian\n",
        "from autograd.misc.flatten import flatten_func\n",
        "\n",
        "#dataset\n",
        "arr = StringIO(\"-1.294595211676430324e-01,1.712677643874767064e-01,1.890068660450803240e-01,6.281774644773006067e-01,9.869838287939920463e-01,1.100000000000000089e+00,1.399999999999999911e+00,4.000000000000000000e+00,4.299999999999999822e+00,4.500000000000000000e+00,4.700000000000000178e+00 \\n0.000000000000000000e+00,0.000000000000000000e+00,0.000000000000000000e+00,0.000000000000000000e+00,0.000000000000000000e+00,1.000000000000000000e+00,1.000000000000000000e+00,1.000000000000000000e+00,1.000000000000000000e+00,1.000000000000000000e+00,1.000000000000000000e+00\")\n",
        "lam = 10**-5 # our regularization parameter\n",
        "\n",
        "data = np.loadtxt(arr,dtype=\"float\",delimiter=\",\")\n",
        "print(np.shape(data))\n",
        "\n",
        "#Gradient: Derivative of a multivariate continuous objective function.\n",
        "#Hessian matrix: Second derivative of a function with two or more input variables.\n",
        "\n",
        "# This is needed to compensate for %matplotlib notebook's tendancy to blow up images when plotted inline\n",
        "%matplotlib notebook\n",
        "from matplotlib import rcParams\n",
        "rcParams['figure.autolayout'] = True\n",
        "\n",
        "\n",
        "# gradient descent function - inputs: g (input function), alpha (steplength parameter), max_its (maximum number of iterations), w (initialization)\n",
        "def gradient_descent(g,alpha_choice,max_its,w):\n",
        "    # flatten the input function to more easily deal with costs that have layers of parameters\n",
        "    g_flat, unflatten, w = flatten_func(g, w) # note here the output 'w' is also flattened\n",
        "\n",
        "    # compute the gradient function of our input function - note this is a function too\n",
        "    # that - when evaluated - returns both the gradient and function evaluations (remember\n",
        "    # as discussed in Chapter 3 we always ge the function evaluation 'for free' when we use\n",
        "    # an Automatic Differntiator to evaluate the gradient)\n",
        "    gradient = value_and_grad(g_flat)\n",
        "\n",
        "    # run the gradient descent loop\n",
        "    weight_history = []      # container for weight history\n",
        "    cost_history = []        # container for corresponding cost function history\n",
        "    alpha = 0\n",
        "    for k in range(1,max_its+1):\n",
        "        # check if diminishing steplength rule used\n",
        "        if alpha_choice == 'diminishing':\n",
        "            alpha = 1/float(k)\n",
        "        else:\n",
        "            alpha = alpha_choice\n",
        "        \n",
        "        # evaluate the gradient, store current (unflattened) weights and cost function value\n",
        "        cost_eval,grad_eval = gradient(w)\n",
        "        weight_history.append(unflatten(w))\n",
        "        cost_history.append(cost_eval)\n",
        "\n",
        "        # take gradient descent step\n",
        "        w = w - alpha*grad_eval\n",
        "            \n",
        "    # collect final weights\n",
        "    weight_history.append(unflatten(w))\n",
        "    # compute final cost function value via g itself (since we aren't computing \n",
        "    # the gradient at the final step we don't get the final cost function value \n",
        "    # via the Automatic Differentiatoor) \n",
        "    cost_history.append(g_flat(w))  \n",
        "    return weight_history,cost_history\n",
        "\n",
        "\n",
        "\n",
        "#linear model\n",
        "# compute linear combination of input point\n",
        "def model(x,w):\n",
        "  a = w[0] + np.dot(x.T,w[1:])\n",
        "  return a.T\n",
        "\n",
        "#log error\n",
        "# define sigmoid function\n",
        "def sigmoid(t):\n",
        "  return 1/(1 + np.exp(-t))\n",
        "\n",
        "# the convex cross-entropy cost function\n",
        "def cross_entropy(w):\n",
        "# compute sigmoid of model\n",
        "  a = sigmoid(model(x,w))\n",
        "\n",
        "# compute cost of label 0 points\n",
        "  ind = np.argwhere(y == 0)[:,1]\n",
        "  cost = -np.sum(np.log(1 - a[:,ind]))\n",
        "\n",
        "# add cost on label 1 points\n",
        "  ind = np.argwhere(y==1)[:,1]\n",
        "  cost -= np.sum(np.log(a[:,ind]))\n",
        "\n",
        "# compute cross-entropy\n",
        "  return cost/y.size\n",
        "\n",
        "\n",
        "# produce static image of gradient descent or newton's method run\n",
        "def static_fig(self,w_hist,**kwargs):\n",
        "        self.w_hist = w_hist\n",
        "        ind = -1\n",
        "        show_path = True\n",
        "        if np.size(w_hist) == 0:\n",
        "            show_path = False\n",
        "        w = 0\n",
        "        if show_path:\n",
        "            w = w_hist[ind]\n",
        "        \n",
        "        ##### setup figure to plot #####\n",
        "        # initialize figure\n",
        "        fig = plt.figure(figsize = (8,3))\n",
        "        artist = fig\n",
        "        \n",
        "        # create subplot with 3 panels, plot input function in center plot\n",
        "        gs = gridspec.GridSpec(1, 2, width_ratios=[1,1]) \n",
        "        ax1 = plt.subplot(gs[0]); \n",
        "        ax2 = plt.subplot(gs[1]);\n",
        "\n",
        "        # produce color scheme\n",
        "        s = np.linspace(0,1,len(self.w_hist[:round(len(self.w_hist)/2)]))\n",
        "        s.shape = (len(s),1)\n",
        "        t = np.ones(len(self.w_hist[round(len(self.w_hist)/2):]))\n",
        "        t.shape = (len(t),1)\n",
        "        s = np.vstack((s,t))\n",
        "        self.colorspec = []\n",
        "        self.colorspec = np.concatenate((s,np.flipud(s)),1)\n",
        "        self.colorspec = np.concatenate((self.colorspec,np.zeros((len(s),1))),1)\n",
        "        \n",
        "        # seed left panel plotting range\n",
        "        xmin = copy.deepcopy(min(self.x))\n",
        "        xmax = copy.deepcopy(max(self.x))\n",
        "        xgap = (xmax - xmin)*0.1\n",
        "        xmin-=xgap\n",
        "        xmax+=xgap\n",
        "        x_fit = np.linspace(xmin,xmax,300)\n",
        "        \n",
        "        # seed right panel contour plot\n",
        "        viewmax = 3\n",
        "        if 'viewmax' in kwargs:\n",
        "            viewmax = kwargs['viewmax']\n",
        "        view = [20,100]\n",
        "        if 'view' in kwargs:\n",
        "            view = kwargs['view']\n",
        "        num_contours = 15\n",
        "        if 'num_contours' in kwargs:\n",
        "            num_contours = kwargs['num_contours']   \n",
        "            \n",
        "        ### contour plot in right panel ###\n",
        "        self.contour_plot(ax2,viewmax,num_contours)\n",
        "        \n",
        "        ### make left panel - plot data and fit ###\n",
        "        # scatter data\n",
        "        self.scatter_pts(ax1)\n",
        "        \n",
        "        if show_path:\n",
        "            # initialize fit\n",
        "            y_fit = self.sigmoid(w[0] + x_fit*w[1])\n",
        "\n",
        "            # plot fit to data\n",
        "            color = self.colorspec[-1]\n",
        "            ax1.plot(x_fit,y_fit,color = color,linewidth = 2) \n",
        "\n",
        "            # add points to right panel contour plot\n",
        "            num_frames = len(self.w_hist)\n",
        "            for k in range(num_frames):\n",
        "                # current color\n",
        "                color = self.colorspec[k]\n",
        "\n",
        "                # current weights\n",
        "                w = self.w_hist[k]\n",
        "\n",
        "                ###### make right panel - plot contour and steps ######\n",
        "                if k == 0:\n",
        "                    ax2.scatter(w[0],w[1],s = 90,facecolor = color,edgecolor = 'k',linewidth = 0.5, zorder = 3)\n",
        "                if k > 0 and k < num_frames:\n",
        "                    self.plot_pts_on_contour(ax2,k,color)\n",
        "                if k == num_frames -1:\n",
        "                    ax2.scatter(w[0],w[1],s = 90,facecolor = color,edgecolor = 'k',linewidth = 0.5, zorder = 3)\n",
        "        \n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# This code cell will not be shown in the HTML version of this notebook\n",
        "# take input/output pairs from data\n",
        "x = data[:-1,:]\n",
        "y = data[-1:,:] \n",
        "\n",
        "# run gradient descent to minimize the softmax cost\n",
        "g = cross_entropy; w = np.array([3.0,3.0])[:,np.newaxis]; max_its = 100; alpha_choice = 10**(0);\n",
        "weight_history,cost_history = gradient_descent(g,alpha_choice,max_its,w)\n",
        "\n",
        "# This code cell will not be shown in the HTML version of this notebook\n",
        "# run gradient descent to minimize the softmax cost\n",
        "g = cross_entropy; w = np.array([3.0,3.0])[:,np.newaxis]; max_its = 2000; alpha_choice = 1;\n",
        "weight_history,cost_history = gradient_descent(g,alpha_choice,max_its,w)\n",
        "\n",
        "# create a static figure illustrating gradient descent steps static_fig(weight_history,num_contours = 25,viewmax = 12)"
      ]
    }
  ]
}