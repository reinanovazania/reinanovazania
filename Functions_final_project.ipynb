{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP99vz2kmKNfKq4PB7fc1h3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reinanovazania/reinanovazania/blob/main/Functions_final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dyGKBjO9nfQd"
      },
      "outputs": [],
      "source": [
        "#https://github.com/WalterjhShen/EECS495_course_proj/blob/master/Functions_final_project.py\n",
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "from autograd import grad\n",
        "import math\n",
        "from autograd.misc.flatten import flatten_func\n",
        "from autograd import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "def batch_gradient_descent(g,x_sample,y_sample,alpha,max_its,w,batch_size,full_size):\n",
        "    g_flat, unflatten_func, w_flat = flatten_func(g, w)\n",
        "    gradient = grad(g_flat)\n",
        "\n",
        "    # run the gradient descent loop\n",
        "    weight_history = [w]\n",
        "    epoch_history = [g_flat(w,x_sample,y_sample,np.arange(full_size))]           # container for epoch cost function history\n",
        "    cost_history = [g_flat(w,x_sample,y_sample,np.arange(full_size))]          # container for corresponding cost function history\n",
        "    for k in range(max_its):\n",
        "        for j in range(math.ceil(float(full_size)/float(batch_size))):\n",
        "            index = np.arange(j*batch_size,min((j+1)*batch_size,full_size))\n",
        "            grad_eval = gradient(w,x_sample,y_sample,index)\n",
        "            w = w - alpha*grad_eval\n",
        "            cost_history.append(g_flat(w,x_sample,y_sample,np.arange(full_size)))\n",
        "        epoch_history.append(g_flat(w,x_sample,y_sample,np.arange(full_size)))\n",
        "        weight_history.append(w)\n",
        "    return weight_history,epoch_history,cost_history\n",
        "\n",
        "# compute C linear combinations of input point, one per classifier\n",
        "def model(x,w):\n",
        "    a = w[0] + np.dot(x.T,w[1:])\n",
        "    return a.T\n",
        "\n",
        "# multiclass perceptron\n",
        "def multiclass_perceptron(w,x,y,iter):\n",
        "    # get subset of points\n",
        "    x_p = x[:,iter]\n",
        "    y_p = y[:,iter]\n",
        "\n",
        "    # pre-compute predictions on all points\n",
        "    all_evals = model(x_p,w)\n",
        "\n",
        "    # compute maximum across data points\n",
        "    a =  np.max(all_evals,axis = 0)        \n",
        "\n",
        "    # compute cost in compact form using numpy broadcasting\n",
        "    b = all_evals[y_p.astype(int).flatten(),np.arange(np.size(y_p))]\n",
        "    cost = np.sum(a - b)\n",
        "\n",
        "    # return average\n",
        "    return cost/float(np.size(y_p))\n",
        "\n",
        "def misclassification(x,y,w):\n",
        "    all_evals = model(x,w)\n",
        "    idx = np.argmax(all_evals,axis = 0)\n",
        "    correct_numbers = np.sum(idx==y)/y.shape[1]\n",
        "    return ((1.0-correct_numbers)*y.shape[1])\n",
        "\n",
        "def get_mis(weight_history_1,x_sample,y_sample):\n",
        "    mis_1=[]\n",
        "    for i in range(len(weight_history_1)):\n",
        "        mis_1.append(misclassification(x_sample,y_sample,weight_history_1[i]))\n",
        "    return(mis_1,(y_sample.shape[1]-mis_1[-1])/y_sample.shape[1])\n",
        "\n",
        "def plot_batch(epoch_history_1,epoch_history_2,mis_1,mis_2,pre_1,pre_2):\n",
        "    fig = plt.figure(figsize=(15,5))\n",
        "\n",
        "    ax1 = plt.subplot(131)\n",
        "    ax1.set_xlabel('full epochs')\n",
        "    ax1.set_ylabel('$g(w^k)$')\n",
        "    ax1.plot(epoch_history_1,markersize=10,label='full batch')\n",
        "    ax1.plot(epoch_history_2,markersize=10,label='minibatch-size=200')\n",
        "    ax1.set_title('cost history_perceptron')\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2 = plt.subplot(132)\n",
        "    ax2.plot(mis_1,label='full batch')\n",
        "    ax2.plot(mis_2,label='minibatch-size=200')\n",
        "    ax2.set_xlabel('full epochs')\n",
        "    ax2.set_ylabel('misclassification number')\n",
        "    ax2.set_title('misclassification history')\n",
        "    ax2.legend()\n",
        "\n",
        "    ax3 = plt.subplot(133)\n",
        "    ax3.plot(pre_1,label='full batch')\n",
        "    ax3.plot(pre_2,label='minibatch-size=200')\n",
        "    ax3.set_xlabel('full epochs')\n",
        "    ax3.set_ylabel('misclassification number')\n",
        "    ax3.set_title('prediction misclassification history')\n",
        "    ax3.legend()\n",
        "\n",
        "    return(plt.show())\n",
        "\n",
        "\n",
        "def gradient_descent(g,alpha,max_its,w,batch_size,full_size):\n",
        "    g_flat, unflatten_func, w_flat = flatten_func(g, w)\n",
        "    gradient = grad(g_flat)\n",
        "\n",
        "    # run the gradient descent loop\n",
        "    weight_history = [w]\n",
        "    cost_history = [g_flat(w,np.arange(full_size))]          # container for corresponding cost function history\n",
        "    for k in range(max_its):\n",
        "        for j in range(math.ceil(float(full_size)/float(batch_size))):\n",
        "            index = np.arange(j*batch_size,min((j+1)*batch_size,full_size))\n",
        "            grad_eval = gradient(w,index)\n",
        "            w = w - alpha*grad_eval\n",
        "            cost_history.append(g_flat(w,np.arange(full_size)))\n",
        "        weight_history.append(w)\n",
        "    return weight_history,cost_history\n",
        "\n",
        "# multiclass softmaax cost\n",
        "def multiclass_softmax(w,x,y,iter):\n",
        "    x_p = x[:,iter]\n",
        "    y_p = y[:,iter]\n",
        "    # pre-compute predictions on all points\n",
        "    all_evals = model(x_p,w)\n",
        "    \n",
        "    # compute softmax across data points\n",
        "    a = np.log(np.sum(np.exp(all_evals),axis = 0)) \n",
        "    \n",
        "    # compute cost in compact form using numpy broadcasting\n",
        "    b = all_evals[y_p.astype(int).flatten(),np.arange(np.size(y_p))]\n",
        "    cost = np.sum(a - b)\n",
        "    \n",
        "    # return average\n",
        "    return cost/float(np.size(y_p))\n",
        "\n",
        "# standard normalization function \n",
        "def standard_normalizer(x):\n",
        "    # compute the mean and standard deviation of the input\n",
        "    x_means = np.mean(x,axis = 1)[:,np.newaxis]\n",
        "    x_stds = np.std(x,axis = 1)[:,np.newaxis]   \n",
        "    x_stds[x_stds==0] = 1\n",
        "\n",
        "    # create standard normalizer function\n",
        "    normalizer = lambda data: (data - x_means)/x_stds\n",
        "\n",
        "    # create inverse standard normalizer\n",
        "    inverse_normalizer = lambda data: data*x_stds + x_means\n",
        "\n",
        "    # return normalizer \n",
        "    return normalizer\n",
        "\n",
        "# compute eigendecomposition of data covariance matrix for PCA transformation\n",
        "def PCA(x,**kwargs):\n",
        "    # regularization parameter for numerical stability\n",
        "    lam = 10**(-7)\n",
        "    if 'lam' in kwargs:\n",
        "        lam = kwargs['lam']\n",
        "\n",
        "    # create the correlation matrix\n",
        "    P = float(x.shape[1])\n",
        "    Cov = 1/P*np.dot(x,x.T) + lam*np.eye(x.shape[0])\n",
        "\n",
        "    # use numpy function to compute eigenvalues / vectors of correlation matrix\n",
        "    d,V = np.linalg.eigh(Cov)\n",
        "    return d,V\n",
        "\n",
        "# PCA-sphereing - use PCA to normalize input features\n",
        "def PCA_sphereing(x,**kwargs):\n",
        "    # Step 1: mean-center the data\n",
        "    x_means = np.mean(x,axis = 1)[:,np.newaxis]\n",
        "    x_centered = x - x_means\n",
        "\n",
        "    # Step 2: compute pca transform on mean-centered data\n",
        "    d,V = PCA(x_centered,**kwargs)\n",
        "\n",
        "    # Step 3: divide off standard deviation of each (transformed) input, \n",
        "    # which are equal to the returned eigenvalues in 'd'.  \n",
        "    stds = (d[:,np.newaxis])**(0.5)\n",
        "    normalizer = lambda data: np.dot(V.T,data - x_means)/stds\n",
        "\n",
        "    # create inverse normalizer\n",
        "    inverse_normalizer = lambda data: np.dot(V,data*stds) + x_means\n",
        "\n",
        "    # return normalizer \n",
        "    return normalizer\n",
        "\n",
        "def plot_result(cost_history_1,cost_history_2,cost_history_3,mis_1,mis_2,mis_3,pre_1,pre_2,pre_3):\n",
        "    \n",
        "    fig = plt.figure(figsize=(15,5))\n",
        "\n",
        "    ax1 = plt.subplot(131)\n",
        "    ax1.set_xlabel('step k')\n",
        "    ax1.set_ylabel('$g(w^k)$')\n",
        "    ax1.plot(cost_history_1,markersize=10,label='original')\n",
        "    ax1.plot(cost_history_2,markersize=10,label='standard')\n",
        "    ax1.plot(cost_history_3,markersize=10,label='sphered')\n",
        "    ax1.set_title('cost history')\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2 = plt.subplot(132)\n",
        "    ax2.plot(mis_1,label='original')\n",
        "    ax2.plot(mis_2,label='standard')\n",
        "    ax2.plot(mis_3,label='sphered')\n",
        "    ax2.set_xlabel('step k')\n",
        "    ax2.set_ylabel('misclassification')\n",
        "    ax2.set_title('misclassification history')\n",
        "    ax2.legend()\n",
        "\n",
        "    ax3 = plt.subplot(133)\n",
        "    ax3.plot(pre_1,label='original')\n",
        "    ax3.plot(pre_2,label='standard')\n",
        "    ax3.plot(pre_3,label='sphered')\n",
        "    ax3.set_xlabel('step k')\n",
        "    ax3.set_ylabel('misclassification')\n",
        "    ax3.set_title('misclassification history')\n",
        "    ax3.legend()\n",
        "\n",
        "    return(plt.show())"
      ]
    }
  ]
}